{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  先前的版本未考慮到每一個病人不一定符合最大天數的條件，導致個別天數的資料集歸人數量不同\n",
    "#### 當前處理方式是以最長的輸入時間(e.g:12天)的病人過濾其他天數資料集，已達成每個資料集的歸人數量皆相同\n",
    "#### 先建構最長天數的資料集後再建構其他的\n",
    "#### new: 建構預測長度非MS架構的數據，e.g: 8,49 => 2,49 or 4,49"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random as rd\n",
    "import warnings, glob, os, sys\n",
    "from tqdm.notebook import tqdm\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([1, 2, 3, 4, 5], dtype='int64')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df = pd.read_pickle('./dataset/第一階段處理結果-FinalPeriHDDataFrame.pkl') #Stage1 NLP分類結果\n",
    "data_df.index[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ID', '洗腎紀錄時間', '洗腎紀錄時間去時分', '急診門診住院', '入院方式', 'FISTULA', 'GRAFT', 'DLC', 'PERMCATH', '處置其他結束', '開始血壓SBP', '開始血壓DBP', '開始脈搏', '體溫', '結束脈搏', 'Catheter', 'K', 'HGB', 'HCT', 'RBC', 'WBC', 'Platelet', 'Creatinine', 'MCV', 'MCHC', 'MCH', 'Na', 'Ca', 'BUN', 'AST (SGOT)', 'ALT (SGPT)', 'Alk.phosphatase', 'Glucose AC', 'ALBUMIN', 'P', 'TIBC', 'Ferritin', 'BUN (洗腎後)', 'IRON/TIBC', 'Intact PTH', 'Iron', 'Bilirubin-T', 'Cholesterol-T', 'CRP', 'HBs', 'HBsAg', 'HBc', 'HCV', '透析液 Ca', 'HBV', '生日', '性別', '體重1開始', '體重2標準體重', '體重結束', '體重實際脫水', '體重機器顯示', '體重3應脫水']\n"
     ]
    }
   ],
   "source": [
    "print(data_df.columns.drop(['age','URR%','算年齡','分母體重', '每公斤脫水量(ml/kg)', '有無糖尿病', '處置其他', '症狀處置','sbp', 'dbp','Raw Index', 'New Intra SBP', 'New Intra DBP', 'New 處置其他+症狀處置', 'New 處置其他結束', 'Max Diff mbp', 'Max Diff sbp', 'NLP Judge', 'BP Judge', 'Final Judge']).to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特徵篩選"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = './dataset/Stage2_EMRs_IDH_csv-05-28_90days-lv3/'\n",
    "os.makedirs(root_path, exist_ok=True) #創建儲存路徑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 過濾非生理數據與未使用特徵\n",
    "data_df.columns\n",
    "drop_cols = ['洗腎紀錄時間', '急診門診住院', '生日', '算年齡', \n",
    "             'DLC', 'PERMCATH', 'HBs', 'HBsAg', 'HBc', \n",
    "             \n",
    "             '體重2標準體重', '體重3應脫水', '體重結束', '分母體重', '體重機器顯示', \n",
    "             'BUN (洗腎後)', \n",
    "             'sbp', 'dbp', 'New Intra SBP', 'New Intra DBP',\n",
    "             'New 處置其他+症狀處置', 'New 處置其他結束', '處置其他結束', '處置其他', '症狀處置', 'NLP Judge', 'BP Judge']\n",
    "\n",
    "# 'Catheter' = 'DLC' + 'PERMCATH'\n",
    "# 'HBs', 'HBsAg', 'HBc'用於判斷HBV、HCV\n",
    "# 洗腎後紀錄：'Max Diff mbp'、'Max Diff sbp'、'每公斤脫水量(ml/kg)'、'結束脈搏'\n",
    "# URR% => BUN、BUN(洗後)\n",
    "# 可以考慮移除'Bilirubin-T'\n",
    "select_cols = ['Raw Index', 'ID', '洗腎紀錄時間去時分', 'age', '開始血壓SBP', '開始血壓DBP', '開始脈搏', '體重1開始',\n",
    "               '入院方式', '性別', '體溫', 'FISTULA', 'GRAFT', 'Catheter', '有無糖尿病','Intact PTH', 'HCV', 'HBV',#catagical data\n",
    "               'K', 'HGB', 'HCT', 'RBC', 'WBC', 'Platelet',\n",
    "               'Creatinine', 'MCV', 'MCHC', 'MCH', 'Na', 'Ca', 'BUN', 'AST (SGOT)',\n",
    "               'ALT (SGPT)', 'Alk.phosphatase', 'Glucose AC', 'ALBUMIN', 'P', 'TIBC',\n",
    "               'Ferritin', 'IRON/TIBC', 'Iron',\n",
    "               'Bilirubin-T', 'Cholesterol-T', 'CRP', 'URR%','透析液 Ca', \n",
    "               'Max Diff mbp', 'Max Diff sbp', '每公斤脫水量(ml/kg)', '結束脈搏', # 治療後數據\n",
    "               '體重實際脫水', 'Final Judge',# label: value target, catagical target\n",
    "               ]\n",
    "filter_data_df = data_df[select_cols]\n",
    "# 檢查特徵範圍和遺失值\n",
    "check_miss_df = {'Feature':[], 'Max':[], 'Min':[], 'Null':[]}\n",
    "for cols_ in filter_data_df.columns:\n",
    "    feature = filter_data_df[cols_].values\n",
    "    check_miss_df[\"Feature\"].append(cols_)\n",
    "    check_miss_df[\"Min\"].append(min(feature[feature!=9999]))\n",
    "    check_miss_df[\"Max\"].append(max(feature[feature!=9999]))\n",
    "    check_miss_df[\"Null\"].append(feature[feature==9999].shape[0])\n",
    "pd.DataFrame(check_miss_df).to_csv('./dataset/最終數據檢查表.csv', encoding='utf-8-sig', index=False)\n",
    "save_path = './original_data/'\n",
    "os.makedirs(os.path.join(root_path, save_path), exist_ok=True) #創建儲存路徑\n",
    "filter_data_df.to_csv(os.path.join(root_path, save_path,'FinalProcess_df.csv'), encoding='utf-8-sig', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 資料分群 - CV [先切Train和Test，Valid從Train中切分]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==初始資料切割狀況==\n",
      "True: 36347.0 (15.37)% False: 200167.0 (84.63)%\n",
      "True: 4702.0 (15.68)% False: 25276.0 (84.32)%\n"
     ]
    }
   ],
   "source": [
    "# 時間序列堆疊，先以groupy方式根據病人ID進行逐筆堆疊\n",
    "filter_data_df['洗腎紀錄時間去時分'] = pd.to_datetime(filter_data_df['洗腎紀錄時間去時分']).dt.date\n",
    "filter_data_df['ID'] = filter_data_df['ID'].astype(str)\n",
    "group_ID_filter_data_df = filter_data_df.groupby(['ID'])\n",
    "ID_list = filter_data_df['ID'].drop_duplicates().to_list()\n",
    "total_length = len(ID_list)\n",
    "train_length = int(0.75 * total_length)\n",
    "val_length = int(0.15 * total_length)\n",
    "rd.seed(24)\n",
    "rd.shuffle(ID_list)\n",
    "train_list = ID_list[:train_length+val_length]\n",
    "test_list = ID_list[train_length+val_length:]\n",
    "print(\"==初始資料切割狀況==\")\n",
    "train_df = filter_data_df[filter_data_df['ID'].isin(train_list)]\n",
    "# train_df = train_df[train_df['Timelag']==-1]\n",
    "print(f\"True: {train_df['Final Judge'].sum()} ({round(train_df['Final Judge'].sum()/len(train_df)*100,2)})% False: {len(train_df)-train_df['Final Judge'].sum()} ({round((len(train_df)-train_df['Final Judge'].sum())/len(train_df)*100,2)})%\")\n",
    "test_df = filter_data_df[filter_data_df['ID'].isin(test_list)]\n",
    "# test_df = test_df[test_df['Timelag']==-1]\n",
    "print(f\"True: {test_df['Final Judge'].sum()} ({round(test_df['Final Judge'].sum()/len(test_df)*100,2)})% False: {len(test_df)-test_df['Final Judge'].sum()} ({round((len(test_df)-test_df['Final Judge'].sum())/len(test_df)*100,2)})%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv(os.path.join(root_path, save_path, 'trainval.csv'), encoding='utf-8-sig', index=False)\n",
    "test_df.to_csv(os.path.join(root_path, save_path, 'test.csv'), encoding='utf-8-sig', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([1, 2, 3, 4, 5], dtype='int64')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_data_df.reset_index(drop=True)\n",
    "# filter_data_df.index = filter_data_df.index+1\n",
    "filter_data_df.index[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 時序資料處理 Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 由於這個步驟全部concat再一起一起做會需要花很多時間，所以先切成歸人csv個別儲存，後續再做合併\n",
    "# 原本需要花17小時 => 修正後5分鐘\n",
    "mask_cols = ['Max Diff mbp','Max Diff sbp', 'Final Judge','結束脈搏']\n",
    "# mask_cols = ['Max Diff mbp','Max Diff sbp','體重實際脫水', 'Final Judge', '每公斤脫水量(ml/kg)','結束脈搏']\n",
    "\n",
    "ths_days = 30 #change 30 => 90\n",
    "stack_nums = 12 #max = 12 / 16 lv.3=16, 8, 4\n",
    "input_nums = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3865449577be40e0b66469213bb21956",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/990 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for idx__, (group_name, group_data) in enumerate(tqdm(group_ID_filter_data_df)):\n",
    "    timeseries_df = pd.DataFrame([], columns=filter_data_df.columns)\n",
    "    timeseries_df['Group Type'] = np.nan #標記Train、Valid、Test\n",
    "    group_data = group_data.sort_values(by=['洗腎紀錄時間去時分'], ascending=True)\n",
    "    group_data = group_data.reset_index(drop=True)\n",
    "    group_data_type = None\n",
    "    for idx in group_data.index:\n",
    "        # print(idx)\n",
    "        try:\n",
    "            slice_df = group_data.loc[idx:idx+(stack_nums-1)].copy()\n",
    "            temp_df = slice_df\n",
    "            if slice_df.shape[0]==stack_nums and (abs(slice_df['洗腎紀錄時間去時分'].iloc[0] - slice_df['洗腎紀錄時間去時分'].iloc[-1]).days<=ths_days): #判斷資料是否有{stack_nums}筆，以及區間時間是否小於等於{ths_days}\n",
    "                if stack_nums!=input_nums:\n",
    "                    slice_df = slice_df.iloc[(stack_nums-input_nums)::]\n",
    "                    slice_df = pd.concat([slice_df,slice_df.iloc[-1].to_frame().T.copy()])\n",
    "                else: \n",
    "                    slice_df = pd.concat([slice_df,slice_df.iloc[-1].to_frame().T.copy()])\n",
    "                slice_df.iloc[-2,slice_df.columns.get_indexer(mask_cols)] = -9999\n",
    "                # slice_df = pd.concat([slice_df,slice_df.iloc[-1].to_frame().T.copy()]) #堆疊兩次\n",
    "                if group_name[0] in test_list:\n",
    "                    slice_df['Group Type'] = 'Test'\n",
    "                    group_data_type = 'Test'\n",
    "                else:\n",
    "                    slice_df['Group Type'] = 'TrainVal'\n",
    "                    group_data_type = 'TrainVal'\n",
    "                timeseries_df = pd.concat([timeseries_df,slice_df])\n",
    "        except:\n",
    "            pass\n",
    "            # break\n",
    "    # break\n",
    "    if timeseries_df.shape[0]!=0:\n",
    "        if stack_nums!=input_nums:\n",
    "            ts_save_path = os.path.join(root_path,f'time_series_{input_nums-1}_patient_cv')\n",
    "        else:\n",
    "            ts_save_path = os.path.join(root_path,f'time_series_{stack_nums-1}_patient_cv')\n",
    "        os.makedirs(ts_save_path, exist_ok=True) #創建儲存路徑\n",
    "        timeseries_df.to_csv(os.path.join(ts_save_path,f'{group_data_type}-{str(group_name[0])}.csv'),encoding='utf-8-sig', index=False)\n",
    "# %%\n",
    "stack_nums = input_nums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 歸人資料合併"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_data_df = pd.read_csv(os.path.join(root_path, 'original_data', 'FinalProcess_df.csv'))\n",
    "filter_data_df_cols = filter_data_df.columns\n",
    "del filter_data_df\n",
    "\n",
    "patient_csv_glob = glob.glob(os.path.join(root_path, f'time_series_{stack_nums-1}_patient_cv','*'))\n",
    "\n",
    "save_merge_path = os.path.join(root_path, f'time_series_{stack_nums-1}_cv')\n",
    "os.makedirs(save_merge_path, exist_ok=True) #創建儲存路徑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b74ab72496be47fc8f40b6f26db12449",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/717 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(258255, 53)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 處理test資料，不需要做cv\n",
    "test_df = pd.DataFrame([], columns=filter_data_df_cols)\n",
    "for csv_path in tqdm(patient_csv_glob):\n",
    "    if 'Test' in csv_path:\n",
    "        data_df = pd.read_csv(csv_path)\n",
    "        test_df = pd.concat([test_df,data_df])\n",
    "        del data_df\n",
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_name = f'timeseries-{stack_nums-1}.csv'\n",
    "test_df.to_csv(os.path.join(save_merge_path, 'test_'+save_name), encoding='utf-8-sig', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bdb99f8c74c4734baaef28733e0efeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/717 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainval_df = pd.DataFrame([], columns=filter_data_df_cols)\n",
    "for csv_path in tqdm(patient_csv_glob):\n",
    "    if 'TrainVal' in csv_path:\n",
    "        data_df = pd.read_csv(csv_path)\n",
    "        trainval_df = pd.concat([trainval_df,data_df])\n",
    "        del data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2027070, 53)\n",
      "(225230, 53) 225230\n"
     ]
    }
   ],
   "source": [
    "trainval_df = trainval_df.reset_index(drop=True)\n",
    "trainval_df['ID'] = trainval_df['ID'].astype(str)\n",
    "trainval_df.index = trainval_df.index+1\n",
    "print(trainval_df.shape)\n",
    "get_target_rows_index = [idx_ for idx_ in range(0,len(trainval_df)+stack_nums, (stack_nums+1))][1::]\n",
    "target_df = trainval_df.loc[get_target_rows_index]\n",
    "target_df['ID'] = target_df['ID'].astype(str)\n",
    "print(target_df.shape, len(target_df['ID']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold-1 Train True: 28091.0 (15.58)% False: 152163.0 (84.42)%\n",
      "Patient nums: 510\n",
      "Fold-1 Valid True: 6937.0 (15.42)% False: 38039.0 (84.58)%\n",
      "Patient nums: 135\n",
      "Fold-2 Train True: 28043.0 (15.55)% False: 152245.0 (84.45)%\n",
      "Patient nums: 523\n",
      "Fold-2 Valid True: 6985.0 (15.54)% False: 37957.0 (84.46)%\n",
      "Patient nums: 122\n",
      "Fold-3 Train True: 28056.0 (15.56)% False: 152231.0 (84.44)%\n",
      "Patient nums: 522\n",
      "Fold-3 Valid True: 6972.0 (15.51)% False: 37971.0 (84.49)%\n",
      "Patient nums: 123\n",
      "Fold-4 Train True: 27917.0 (15.51)% False: 152092.0 (84.49)%\n",
      "Patient nums: 521\n",
      "Fold-4 Valid True: 7111.0 (15.72)% False: 38110.0 (84.28)%\n",
      "Patient nums: 124\n",
      "Fold-5 Train True: 28005.0 (15.55)% False: 152077.0 (84.45)%\n",
      "Patient nums: 504\n",
      "Fold-5 Valid True: 7023.0 (15.56)% False: 38125.0 (84.44)%\n",
      "Patient nums: 141\n",
      "(2027070, 58)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "sgkf = StratifiedGroupKFold(n_splits=5)\n",
    "temp_trainval_df = trainval_df.copy()\n",
    "unique_ID = {value: idx  for idx, value in enumerate(target_df['ID'].unique())}\n",
    "target_df['group_ID'] = target_df['ID'].apply(lambda x: unique_ID[x])\n",
    "for fold, (train_index, valid_index) in enumerate(sgkf.split(target_df, target_df['Final Judge'], target_df.group_ID)):\n",
    "    temp_trainval_df[f'fold_{fold+1}'] = 'Train'\n",
    "    \n",
    "    choice_ID = target_df.iloc[train_index]\n",
    "    print(f\"Fold-{fold+1} Train True: {choice_ID['Final Judge'].sum()} ({round(choice_ID['Final Judge'].sum()/len(choice_ID)*100,2)})% False: {len(choice_ID)-choice_ID['Final Judge'].sum()} ({round((len(choice_ID)-choice_ID['Final Judge'].sum())/len(choice_ID)*100,2)})%\")\n",
    "    choice_ID = choice_ID.ID.unique()\n",
    "    print(f\"Patient nums: {len(choice_ID)}\")\n",
    "    \n",
    "    choice_ID = target_df.iloc[valid_index]\n",
    "    print(f\"Fold-{fold+1} Valid True: {choice_ID['Final Judge'].sum()} ({round(choice_ID['Final Judge'].sum()/len(choice_ID)*100,2)})% False: {len(choice_ID)-choice_ID['Final Judge'].sum()} ({round((len(choice_ID)-choice_ID['Final Judge'].sum())/len(choice_ID)*100,2)})%\")\n",
    "    choice_ID = choice_ID.ID.unique()\n",
    "    print(f\"Patient nums: {len(choice_ID)}\")\n",
    "    valid_df_idx = trainval_df[trainval_df['ID'].isin(choice_ID)].copy().index\n",
    "    \n",
    "    temp_trainval_df.loc[valid_df_idx,f'fold_{fold+1}'] = 'Valid'\n",
    "\n",
    "# if stack_nums!=12:\n",
    "#     filter_ID_train = pd.read_csv('./dataset/Stage2_EMRs_IDH_csv-05-28/time_series_11_cv/trainval_timeseries-11.csv').astype(str)['ID'].unique()\n",
    "#     temp_trainval_df = temp_trainval_df[temp_trainval_df['ID'].str.isin(filter_ID_train.tolist())]\n",
    "\n",
    "save_name = f'trainval_timeseries-{stack_nums-1}.csv'\n",
    "print(temp_trainval_df.shape)\n",
    "temp_trainval_df.to_csv(os.path.join(save_merge_path, save_name), encoding='utf-8-sig', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import StratifiedGroupKFold\n",
    "# sgkf = StratifiedGroupKFold(n_splits=5)\n",
    "# # target_df['group_counts']\n",
    "# unique_ID = {value: idx  for idx, value in enumerate(target_df['ID'].unique())}\n",
    "# target_df['group_ID'] = target_df['ID'].apply(lambda x: unique_ID[x])\n",
    "# for fold, (train_index, valid_index) in enumerate(sgkf.split(target_df, target_df['Final Judge'], target_df.group_ID)):\n",
    "    \n",
    "#     choice_ID = target_df.iloc[train_index]\n",
    "#     print(f\"Fold-{fold+1} Train True: {choice_ID['Final Judge'].sum()} ({round(choice_ID['Final Judge'].sum()/len(choice_ID)*100,2)})% False: {len(choice_ID)-choice_ID['Final Judge'].sum()} ({round((len(choice_ID)-choice_ID['Final Judge'].sum())/len(choice_ID)*100,2)})%\")\n",
    "#     choice_ID = choice_ID.ID.unique()\n",
    "#     print(f\"Patient nums: {len(choice_ID)}\")\n",
    "#     train_df = trainval_df[trainval_df['ID'].isin(choice_ID)]\n",
    "#     train_df['Group Type'] = 'Train'    \n",
    "#     print(train_df.shape)\n",
    "    \n",
    "#     choice_ID = target_df.iloc[valid_index]\n",
    "#     print(f\"Fold-{fold+1} Valid True: {choice_ID['Final Judge'].sum()} ({round(choice_ID['Final Judge'].sum()/len(choice_ID)*100,2)})% False: {len(choice_ID)-choice_ID['Final Judge'].sum()} ({round((len(choice_ID)-choice_ID['Final Judge'].sum())/len(choice_ID)*100,2)})%\")\n",
    "#     choice_ID = choice_ID.ID.unique()\n",
    "#     print(f\"Patient nums: {len(choice_ID)}\")\n",
    "#     valid_df = trainval_df[trainval_df['ID'].isin(choice_ID)]\n",
    "#     valid_df['Group Type'] = 'Valid'    \n",
    "#     print(valid_df.shape)\n",
    "    \n",
    "#     concat_train_val = pd.concat([train_df, valid_df])\n",
    "#     save_name = f'fold_{fold+1}_trainval_timeseries-{stack_nums-1}.csv'\n",
    "#     print(concat_train_val.shape)\n",
    "#     concat_train_val.to_csv(os.path.join(save_merge_path, save_name), encoding='utf-8-sig', index=False)\n",
    "#     # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mac_pt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
